{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0d1368",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU Advanced Prompt Engineering for LLMs</a>\n",
    "## <a name=\"0\">Lab 4: Reason and Act (ReAct)</a>\n",
    "\n",
    "This notebook introduces ReAct, a general paradigm that combines reasoning and acting with LLMs. ReAct prompts LLMs to generate verbal reasoning traces and actions for a task. This allows the system to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia, code interpreters) to incorporate additional information into the reasoning. \n",
    "\n",
    "1. <a href=\"#1\">Import libraries</a>\n",
    "2. <a href=\"#2\">Set up Bedrock for inference</a>\n",
    "3. <a href=\"#3\">Reason and Act (ReAct)</a>\n",
    "    - <a href=\"#31\">LangChain tools</a>\n",
    "    - <a href=\"#32\">LangChain agents</a>\n",
    "4. <a href=\"#4\">ReAct examples</a>\n",
    "    - <a href=\"#41\">Time awareness</a>\n",
    "    - <a href=\"#42\">Calculator and code interpreter</a>\n",
    "    - <a href=\"#43\">Knowledge-intensive tasks</a>\n",
    "    - <a href=\"#44\">Decision-making tasks</a>\n",
    "    \n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "You will be presented with coding activities to check your knowledge and understanding throughout the notebook whenever you see the MLU robot:\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3020b5",
   "metadata": {},
   "source": [
    "### <a name=\"1\">1. Import libraries</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's start by installing all required packages as specified in the `requirements.txt` file and importing several libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dc09b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U pip --root-user-action=ignore\n",
    "!pip3 install -q -r requirements.txt --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b81748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, time\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun, tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import AgentType, Tool, load_tools, initialize_agent\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21e229-b076-4472-bb42-549e79738b3c",
   "metadata": {},
   "source": [
    "### <a name=\"2\">2. Set up Bedrock for inference</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To get started, set up Bedrock and instantiate an active `bedrock-runtime` to query LLMs. The code below leverages [LangChain's Bedrock integration](https://python.langchain.com/docs/integrations/llms/bedrock).\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>A note on Bedrock API invocation</h4>\n",
    "\n",
    "Amazon Bedrock is generally available for all AWS customers. The provided lab for this course currently invokes Bedrock's external endpoint, which has access to Anthropic and AI21 models.\n",
    "\n",
    "For training and learning purposes, Amazon Bedrock is also available through an internal endpoint that allows accessing Amazon's Titan models. To use that instead, please follow the instructions in the <a href=\"https://w.amazon.com/bin/view/AmazonBedrock/Products/GetStarted/\">Amazon Bedrock Get Started wiki page</a>.\n",
    "</div>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7498ed-6091-46f5-a013-e1f5515cd6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEYS_INFERENCE = {\n",
    "    \"anthropic\": {\n",
    "        \"max_tokens\": \"max_tokens_to_sample\",\n",
    "        \"stop_sequences\": \"stop_sequences\",\n",
    "    },\n",
    "    \"ai21\": {\"max_tokens\": \"maxTokens\", \"stop_sequences\": \"stopSequences\"},\n",
    "    \"amazon\": {\"max_tokens\": \"maxTokenCount\", \"stop_sequences\": \"stopSequences\"}\n",
    "}\n",
    "\n",
    "\n",
    "# Define the bedrock-runtime client that will be used for predictions\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "########################### Select the LLM ###############################\n",
    "\n",
    "# bedrock_model_id = \"ai21.j2-mid-v1\"\n",
    "# bedrock_model_id = \"ai21.j2-ultra-v1\"\n",
    "bedrock_model_id = \"anthropic.claude-instant-v1\"\n",
    "# bedrock_model_id = \"anthropic.claude-v2\"\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# Each model has a different set of inference parameters\n",
    "model_provider = bedrock_model_id.split(\".\")[0]\n",
    "keys = KEYS_INFERENCE[model_provider]\n",
    "\n",
    "inference_modifier = {\n",
    "    \"temperature\": 0.0,\n",
    "    keys[\"max_tokens\"]: 1024,\n",
    "    keys[\"stop_sequences\"]: [],\n",
    "}\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=bedrock_model_id, client=bedrock_runtime, model_kwargs=inference_modifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e39bc-3f4a-4f12-b753-1d8ace2a0d6e",
   "metadata": {},
   "source": [
    "Next, use Bedrock for inference to test everything works as expected. \n",
    "\n",
    "We will first run the example with Claude Instant. Since the Anthropic models have been trained with [special tokens](https://docs.anthropic.com/claude/docs/human-and-assistant-formatting) to indicate the roles of the user and the assistant, we wrap every input to the model with the appropriate format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7ef78f-9ee8-42f9-bad0-53451e940b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " I don't have a definitive answer to the meaning of life. As an AI, I was created by Anthropic to be helpful, harmless, and honest. Different people may have different views on what gives their life meaning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example model prompting\n",
    "\n",
    "claude = lambda st: f\"\\n\\nHuman: {st}\\n\\nAssistant:\"\n",
    "\n",
    "greeting_prompt = \"Hello, what's the meaning of life?\"\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(llm(claude(greeting_prompt))))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709fa4e",
   "metadata": {},
   "source": [
    "### <a name=\"3\">3. Reason and Act (ReAct)</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "ReAct was introduced in [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629), by Yao et al. (2022) to explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner. **Reasoning traces**, elicited e.g. by CoT, help the model induce, track, and update action plans as well as handle exceptions. **Actions**, induced e.g. by action plan generation, allow it to interface with external sources, such as knowledge bases or environments, to gather additional information.\n",
    "\n",
    "The ReAct method involves augmenting prompts with relational constraints, allowing the LM to better capture the desired context. The key steps are as follows:\n",
    "\n",
    "- **Identify entities**: define the entities in the context that are relevant to the task.\n",
    "- **Formulate relations**: determine the relationships between the identified entities. These relations can be explicit (e.g., \"X is a type of Y\") or implicit (e.g., \"X and Y are often found together\").\n",
    "- **Augment prompts**: combine the identified entities and their relations into a structured prompt that guides the LM towards generating contextually relevant response.\n",
    "\n",
    "In general, ReAct outperforms CoT on question answering, fact verification, and decision making tasks. CoT might suffer from fact hallucination, however it is also true that ReAct's structural constraints reduce its flexibility in formulating reasoning steps. Also ReAct depends a lot on the information it's retrieving; non-informative search results might derail the model reasoning and lead to difficulty in recovering and reformulating thoughts.\n",
    "\n",
    "Prompting methods that **combine and support switching between ReAct and CoT+Self-Consistency generally outperform** all the other approaches.\n",
    "\n",
    "Below you will see an example of a Bedrock-hosted LLM improving its capabilities with ReAct via LangChain's built-in functionality that builds agents to perform tasks by combining the power of LLMs with different tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8ccbb",
   "metadata": {},
   "source": [
    "#### <a name=\"31\">3.1. LangChain tools</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "ReAct can be enhanced with specialized agents and tools. These can help in automating the process of identifying entities and relations, as well as in formulating structured prompts. \n",
    "\n",
    "Tools are functions that agents can use to interact with the world. These tools can be generic utilities (e.g. search), other chains, or even other agents. With tools, **LLMs can search the web, do math, run code**, and more. While the LangChain library provides a [substantial selection of pre-built tools](https://python.langchain.com/docs/integrations/tools/), in many real-world projects, we'll often find that existing tool might not work out-of-the-boxi as expected. Eventually we might need to modify existing tools or build entirely new ones.\n",
    "\n",
    "#### Custom Tools\n",
    "\n",
    "Let's start by building a custom tool to give models access to the current date, a piece of information that is very relevant to answer queries about the real world, and that LLMs do not have access to, as they lack awareness of present vs. past time.\n",
    "\n",
    "The tool below returns the current date regardless of the input. Note that we enter the purpose of the tool in its `description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a9536d8-a42d-465e-b910-a60a9ec81e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2024-01-30"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@tool\n",
    "def current_date(text: str) -> str:\n",
    "    \"\"\"Returns today's date, use this for any questions related to knowing today's date. \\\n",
    "    When you use this function, the Action should be current_date. The Action Input should be \"\" \\\n",
    "    This function will always return today's date. \\\n",
    "    Any date math should occur outside this function.\"\"\"\n",
    "    return str(date.today())\n",
    "\n",
    "# Define the date tool\n",
    "date_tool = Tool(\n",
    "    name=\"DateTool\", func=current_date, description=\"Useful to retrieve the current date.\"\n",
    ")\n",
    "\n",
    "# Test date tool\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown((date_tool(\"Whatever I input, the output should be today's date.\"))))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d04e39-f58d-4235-a33b-6d7d19e63c50",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Python interpreter\n",
    "\n",
    "LangChain supports the execution of Python code via the [PythonREPLTool](https://python.langchain.com/docs/integrations/toolkits/python) a tool to access Python's standard shell, or REPL (Read-Eval-Print Loop).\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>A note on security best practices with LLMs</h4>\n",
    "\n",
    "The Python REPL tool is shown here as an example of what's possible to build with ReAct.\n",
    "<br/>\n",
    "This demo does not use or teach security best practices. You should not allow generative AI to run arbitrary code on production systems.</div>\n",
    "\n",
    "\n",
    "#### Web search Tool\n",
    "\n",
    "The tool below uses the `DuckDuckGo` search module to build a tool that allows to run web searches. Other APIs can be used to search on the web, however they might require setting up APIs and other resources, so we will leverage [DuckDuckGo](https://duckduckgo.com/) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620ab828-d0df-4c03-8dda-f01aecc62e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain/utilities/duckduckgo_search.py:43: UserWarning: DDGS running in an async loop. This may cause errors. Use AsyncDDGS instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The official Taylor Swift reputation Stadium Tour tee is available now! Skip the long lines at the venues and show now HERE . Learn about the life and career of Taylor Swift, one of the biggest pop stars and 12-time Grammy winners. From country music to pop music, from awards to controversies, from relationships to activism, discover how she became a pop megastar and a pop culture icon. December 6, 2023 7:38 AM EST. T aylor Swift is telling me a story, and when Taylor Swift tells you a story, you listen, because you know it's going to be good—not only because she's had an ... And by Taylor Swift, we mean her boyfriend, Kansas City Chiefs tight end Travis Kelce, who led his team to victory against the Baltimore Ravens on Sunday night and clinched a spot in Feb. 11's ... David Letterman Says Taylor Swift Is a \"Glowing Bright Light of Goodness\". The former talk show host also expressed his support for Swift and Travis Kelce's relationship. By Christy Piña ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the API wrapper for DuckDuckGo search\n",
    "duckduckgo_search_run = DuckDuckGoSearchRun()\n",
    "\n",
    "# Define the DuckDuckGo tool using a description and the function to retrieve results from DuckDuckGo search\n",
    "duckduckgo_general_tool = Tool(\n",
    "    name=\"DuckDuckGoSearch General\",\n",
    "    func=duckduckgo_search_run.run,\n",
    "    description=\"Useful for when you need to search online about facts and events or retrieve news\"\n",
    ")\n",
    "\n",
    "# Test the DuckDuckGo tool\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(duckduckgo_general_tool(\"Taylor Swift\")))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178be215-0978-45fe-9afa-563f57b017f8",
   "metadata": {},
   "source": [
    "#### Wikipedia Tool\n",
    "\n",
    "The Wikipedia tool allows to retrieve information about people, places, companies, facts, historical events, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6627da0-3350-4f08-87fd-570937a5fd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Page: Airplane\n",
       "Summary: An airplane (American English) or aeroplane (Commonwealth English), informally plane, is a fixed-wing aircraft that is propelled forward by thrust from a jet engine, propeller, or rocket engine. Airplanes come in a variety of sizes, shapes, and wing configurations. The broad spectrum of uses for airplanes includes recreation, transportation of goods and people, military, and research. Worldwide, commercial aviation transports more than four billion passengers annually on airliners and transports more than 200 billion tonne-kilometers of cargo annually, which is less than 1% of the world's cargo movement. Most airplanes are flown by a pilot on board the aircraft, but some are designed to be remotely or computer-controlled such as drones.\n",
       "The Wright brothers invented and flew the first airplane in 1903, recognized as \"the first sustained and controlled heavier-than-air powered flight\". They built on the works of George Cayley dating from 1799, when he set forth the concept of the modern airplane (and later built and flew models and successful passenger-carrying gliders) and the work of German pioneer of human aviation Otto Lilienthal, who, between 1867 and 1896, also studied heavier-than-air flight. Lilienthal's flight attempts in 1891 are seen as the beginning of human flight.\n",
       "Following its limited use in World War I, aircraft technology continued to develop. Airplanes had a presence in all the major battles of World War II. The first jet aircraft was the German Heinkel He 178 in 1939. The first jet airliner, the de Havilland Comet, was introduced in 1952. The Boeing 707, the first widely successful commercial jet, was in commercial service for more than 50 years, from 1958 to at least 2013.\n",
       "\n",
       "\n",
       "\n",
       "Page: Shiva Ayyadurai\n",
       "Summary: V. A. Shiva Ayyadurai (born Vellayappa Ayyadurai Shiva on December 2, 1963) is an Indian-American engineer, politician, entrepreneur, and anti-vaccine activist. He has become known for promoting conspiracy theories, pseudoscience, and unfounded medical claims. Ayyadurai holds four degrees from the Massachusetts Institute of Technology (MIT), including a PhD in biological engineering, and is a Fulbright grant recipient.In a 2011 article published by Time, Ayyadurai claimed to have invented email, as a teenager; in August 1982, he registered the copyright on an email application he had written. Historians strongly dispute this account because email was already in use in the early 1970s. Ayyadurai sued Gawker Media and Techdirt for defamation for disputing his account of inventing email; both lawsuits were settled out of court. Ayyadurai and Techdirt agreed to Techdirt's articles remaining online with a link to Ayyadurai's rebuttal on his own website. Ayyadurai also attracted attention for two reports: the first questioning the working conditions of India's largest scientific agency; the second questioning the safety of genetically modified food, such as soybeans. During the COVID-19 pandemic, Ayyadurai became known for a social media COVID-19 disinformation campaign, spreading conspiracy theories about the cause of COVID-19, promoting unfounded COVID-19 treatments, and campaigning to fire Anthony Fauci for allegedly being a deep state actor.\n",
       "Ayyadurai garnered 3.39% of the vote as an independent candidate in the 2018 U.S. Senate election in Massachusetts, and ran for the Republican Party in the 2020 U.S. Senate election in Massachusetts but lost to Kevin O'Connor in the primary. After the election, he promoted claims of election fraud that were shown to be false by fact checkers.\n",
       "\n",
       "\n",
       "\n",
       "Page: List of inventors killed by their own invention\n",
       "Summary: This is a list of people whose deaths were in some manner caused by or directly related to a product, process, procedure, or other technological innovation that they invented or designed.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Define the API wrapper for Wikipedia\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "\n",
    "# Define the Wikipedia tool with a description and the function to retrieve results from Wikipedia\n",
    "wikipedia_tool = Tool(\n",
    "    name=\"wikipedia\",\n",
    "    func=wikipedia.run,\n",
    "    description=\"Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\",\n",
    ")\n",
    "\n",
    "# Test the defined Wikipedia tool\n",
    "display(Markdown(\"---\"))\n",
    "display(Markdown(wikipedia_tool(\"Who invented the airplane?\")))\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7feff-410b-4d18-8a42-e1202043db6e",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Try creating more tools for other tasks.</b></br> For example, you can ask use <a href=\"https://python.langchain.com/docs/integrations/tools/yahoo_finance_news\">YahooFinanceNewsTool</a> to query for financial information from companies such as <code>AMZN</code>.\n",
    "</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0965f7-a74f-4677-affb-5d53c5e64925",
   "metadata": {},
   "source": [
    "#### <a name=\"32\">3.2. LangChain agents</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "An Agent is a wrapper around a model, which takes in user input and returns a response corresponding to an \"action\" to take and a corresponding \"action input\". With agents, an **LLM is used as a reasoning engine** to determine which actions to take and in which order.\n",
    "\n",
    "LangChain provides a utility to initialize an agent. To do so, we need to pass the following parameters:\n",
    "\n",
    "- **`tools`**: a list of tools that the model has access to\n",
    "- **`llm`**: the language model that will perform the reasoning and act process\n",
    "- **`agent`**: the type of agent that we want to initialize\n",
    "\n",
    "The example below initializes a \"Zero-shot React Description\" agent. Zero-shot means the agent functions on the current action only, i.e. it has no memory. The agent uses the ReAct framework to decide which tool to use, based solely on the tool's `description`. It is important to give the agent access to a correct set of tools, otherwise it will not be able to accomplish its objective. Additionally, if tools are not properly described, the agent won't know how to use them.\n",
    "\n",
    "The code below initializes an agent with the following capabilites:\n",
    "\n",
    "- identify current date\n",
    "- access to Python interpreter\n",
    "- web search capabilities (general, trips, hotels, flights)\n",
    "- wikipedia queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51db8c9-73cb-4ea6-a0dc-9e31cbb02c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    current_date,\n",
    "    PythonREPLTool(),\n",
    "    duckduckgo_general_tool,\n",
    "    wikipedia_tool\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff984678-76a3-460e-aea1-ebb434f5ec3a",
   "metadata": {},
   "source": [
    "### Customizing the agent's prompt template\n",
    "\n",
    "LangChain's Zero-shot React Description agent orchestrates the sequence of thoughts-actions-observations via a default template that is stored in `agent.agent.llm_chain.prompt.template` as you can see below. \n",
    "\n",
    "For more control on the ReAct results, it is possible to **customize the LLM agent's prompt template**, as demonstrated in this [Custom LLM agent walkthrough](https://python.langchain.com/docs/modules/agents/how_to/custom_llm_agent). This way, one can force the agent to output the final answer in a special format or style. Similarly, one can make rules to the LLM explicit in the prompt template, for instance adding an instruction for the LLM to always use a particular tool as the first tool or enforcing a specific order constraint when calling the available tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd52af06-a91f-4efa-863c-65e565dc7692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "current_date: current_date(text: str) -> str - Returns today's date, use this for any questions related to knowing today's date.     When you use this function, the Action should be current_date. The Action Input should be \"\"     This function will always return today's date.     Any date math should occur outside this function.\n",
      "Python_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
      "DuckDuckGoSearch General: Useful for when you need to search online about facts and events or retrieve news\n",
      "wikipedia: Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [current_date, Python_REPL, DuckDuckGoSearch General, wikipedia]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# Default prompt template of AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    "print(agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fe9b4-f830-48e2-a835-1e438274a190",
   "metadata": {},
   "source": [
    "#### Util function to run ReAct queries\n",
    "\n",
    "The util function below will be used to run queries with and without ReAct for easy comparison of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afcb40bd-e3a5-4922-b84e-3e34b54fd343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_standard_and_react(llm, agent, prompt):\n",
    "    \"\"\"\n",
    "    Utility function to prompt an LLM with a standard query and with ReAct through an agent \n",
    "    \"\"\"\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(prompt))\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"### Without ReAct\"))\n",
    "    display(Markdown(llm(prompt)))\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"### With ReAct\"))\n",
    "    display(Markdown(agent.run(prompt)))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46538f22-d171-4e41-b3ef-56240bb2da84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <a name=\"4\">4. ReAct examples</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "It is time to put our agent to work. Let's create an agent with current time and Python capabilities.\n",
    "\n",
    "#### <a name=\"41\">4.1. Time awareness</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The LLM can gain awareness of the current time, since it has access to our custom date tool. Compare the results obtained with the LLM as a ReAct agent versus standard prompting without ReAct:\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px\">\n",
    "    <h4>A note on security best practices with LLMs</h4>\n",
    "\n",
    "The Python REPL tool is used here as an example of what's possible to do with ReAct.\n",
    "<br/>\n",
    "This demo does not use or teach security best practices. You should not allow generative AI to run arbitrary code on production systems.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a8cec6-f98d-43ec-ad41-f896c610797d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Human: I was born on March 22, 2000. How old am I today in years, months and days?\n",
       "\n",
       "Assistant:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Without ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Okay, here are the steps:\n",
       "\n",
       "1) You were born on March 22, 2000\n",
       "2) The current date is May 25, 2022\n",
       "3) To calculate your age in years: 2022 - 2000 = 22 years\n",
       "4) To calculate your age in months: Take the number of full years (22) and multiply by 12 months/year. Then add the number of months since your last birthday. 22 * 12 = 264 months. Since your last birthday was in March, and it's now May, that's 2 additional months. So 264 + 2 = 266 months.  \n",
       "5) To calculate your age in days: Take the number of full years (22) and multiply by 365 days/year. Then add the number of days since your last birthday. 22 * 365 = 8030 days. Since your last birthday was March 22, and it's now May 25, that's 64 additional days. So 8030 + 64 = 8094 days.\n",
       "\n",
       "Therefore, your current age is:\n",
       "22 years\n",
       "266 months \n",
       "8094 days"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### With ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to know today's date to calculate the age\n",
      "Action: current_date\n",
      "Action Input: \"\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2024-01-30\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I can use Python to calculate the difference between the birthdate and today's date\n",
      "Action: Python_REPL\n",
      "Action Input: import datetime\n",
      "from datetime import date\n",
      "\n",
      "birthdate = date(2000, 3, 22)\n",
      "today = date(2024, 1, 30)\n",
      "age = today - birthdate\n",
      "print(age)\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m8714 days, 0:00:00\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the days to years, months and days\n",
      "Action: Python_REPL \n",
      "Action Input: import datetime\n",
      "\n",
      "from datetime import date\n",
      "\n",
      "birthdate = date(2000, 3, 22)\n",
      "today = date(2024, 1, 30)\n",
      "age_in_days = (today - birthdate).days\n",
      "age = today.year - birthdate.year - 1\n",
      "if today < date(today.year, birthdate.month, birthdate.day):\n",
      "    age += 1\n",
      "\n",
      "print(f\"{age} years, {age_in_days//365} months, {age_in_days%365} days\")\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m24 years, 23 months, 319 days\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the age in years, months and days\n",
      "Final Answer: 24 years, 23 months, 319 days\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "24 years, 23 months, 319 days"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = claude(\"I was born on March 22, 2000. How old am I today in years, months and days?\")\n",
    "run_standard_and_react(llm, agent, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bd4c3-1668-4431-9f14-fe2cf7b51bc6",
   "metadata": {},
   "source": [
    "#### <a name=\"42\">4.2. Calculator and Code interpreter</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The ability to run Python code greatly enhances the performance of LLMs in some arithmetic as well as logical tasks with a clear algorithmic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28d2746-0d26-4b08-8d4f-c60eaa00105f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Human: What's the square root of 2.5 million?\n",
       "\n",
       "Assistant:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Without ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here are the step-by-step workings:\n",
       "1) 2.5 million = 2,500,000\n",
       "2) Take the square root of 2,500,000\n",
       "3) The square root of 2,500,000 is 5000\n",
       "\n",
       "Therefore, the square root of 2.5 million is 5000."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### With ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To find the square root of a number, I need to use the math module in Python.\n",
      "Action: Python_REPL\n",
      "Action Input: import math; print(math.sqrt(2500000))\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m1581.1388300841897\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have the numerical value of the square root of 2.5 million.\n",
      "Final Answer: The square root of 2.5 million is approximately 1581.14.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The square root of 2.5 million is approximately 1581.14."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EXAMPLE 1\n",
    "# Arithmetic calculation that an LLM might get wrong\n",
    "\n",
    "prompt = claude(\"What's the square root of 2.5 million?\")\n",
    "run_standard_and_react(llm, agent, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c99460e-079e-46dd-8807-5cc0cc4d3341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Human: Invert the letters of the following words and sort them in reverse order: apple, Banana, Cherry, pear, elderberry\n",
       "\n",
       "Assistant:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Without ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here are the words with inverted letters sorted in reverse order:\n",
       "\n",
       "elderberry => yrerbdel\n",
       "pear => raep\n",
       "Cherry => yrrehC\n",
       "Banana => ananaB \n",
       "apple => elppa"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### With ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to invert the letters of each word and then sort them in reverse alphabetical order\n",
      "Action: Python_REPL\n",
      "Action Input: words = [\"apple\", \"Banana\", \"Cherry\", \"pear\", \"elderberry\"]\n",
      "inverted = [word[::-1] for word in words]\n",
      "print(sorted(inverted, reverse=True))\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m['yrrehC', 'yrrebredle', 'raep', 'elppa', 'ananaB']\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The final answer is the output of running the Python code to invert and sort the letters of the words in reverse order.\n",
      "Final Answer: ['yrrehC', 'yrrebredle', 'raep', 'elppa', 'ananaB']\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "['yrrehC', 'yrrebredle', 'raep', 'elppa', 'ananaB']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EXAMPLE 2\n",
    "# Sorting of words that an LLM migth get wrong\n",
    "\n",
    "prompt = claude(\"Invert the letters of the following words and sort them in reverse order: apple, Banana, Cherry, pear, elderberry\")\n",
    "run_standard_and_react(llm, agent, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6c957-5c3c-47f6-bac7-062c360f215a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Try solving similar tasks involving math and code.</b></br> For example, you can ask the model to compute compound interest over the years, or to answer arithmetic or trigonometric math problems.</code>\n",
    "</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f98200a0-7f53-4787-8383-6da600fca3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005ca96-3ede-4bb4-911b-64a0de417769",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a name=\"43\">4.3. Knowledge-intensive tasks</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will now use an agent enhanced with web search and Wikipedia query capabilites and probe it in its ability to answer questions about facts of the real world. The example below requires certain knowledge about persons and facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ade4cd1-593b-4fad-ab80-06e0437327c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Human: Which AWS service provides access to foundation models?\n",
       "\n",
       "Assistant:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Without ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Amazon Web Services (AWS) provides access to foundation models through Amazon Lex, an AI assistant creation service. With Lex, you can build conversational interfaces into any application using voice and text. Lex uses advanced deep learning technologies like deep neural networks and natural language understanding to enable conversations between humans and bots.\n",
       "\n",
       "Some key points:\n",
       "\n",
       "- Amazon Lex allows developers to build conversational bots or digital assistants with natural language understanding capabilities powered by deep learning technologies.\n",
       "\n",
       "- Lex bots can understand what users say (speech recognition), determine what they mean (natural language understanding), and respond in a natural way (natural language generation).\n",
       "\n",
       "- Under the hood, Lex bots leverage large language models and deep neural networks trained on massive amounts of text, similar to other foundation models.\n",
       "\n",
       "- Developers can build Lex bots through an easy-to-use web interface without having to build, train or deploy their own deep learning models.\n",
       "\n",
       "- Once built, Lex bots can be integrated into applications, devices, and services to enable natural conversations via voice, text, or both.\n",
       "\n",
       "So in summary, the AWS service that provides access to foundation model technologies for building conversational interfaces is Amazon Lex. It handles all the deep learning underpinnings and allows developers to focus on the conversational aspects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### With ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to search online for AWS services\n",
      "Action: DuckDuckGoSearch General\n",
      "Action Input: AWS services for foundation models\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies, including AI21 Labs, Anthropic, Cohere, Stability AI, and Amazon, along with a broad set of capabilities to build generative AI applications, simplifying the development while maintaining privacy and security. This April, Swami Sivasubramanian, Vice President of Data and Machine Learning at AWS, announced Amazon Bedrock and Amazon Titan models as part of new tools for building with generative AI on AWS. Amazon Bedrock, currently available in preview, is a fully managed service that makes foundation models (FMs) from Amazon and leading AI startups—such as […] Amazon Bedrock is a new service for building and scaling generative AI applications, which are applications that can generate text, images, audio, and synthetic data in response to prompts. Amazon Bedrock gives customers easy access to foundation models (FMs)—those ultra-large ML models that generative AI relies on—from the top AI startup ... Amazon Bedrock, a fully managed service, gives you access to foundation models from leading foundation models including Amazon's own Titan family of models, available via a fully managed API. Amazon SageMaker JumpStart is self-service machine learning hub. It offers built-in algorithms, pre-trained foundation models, and easy-to-use solutions ... Generative AI is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. Like all AI, generative AI is powered by ML models—very large models that are pre-trained on vast amounts of data and commonly referred to as Foundation Models (FMs). Recent advancements in ML (specifically the ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the search results, Amazon Bedrock seems to be the AWS service that provides access to foundation models.\n",
      "Final Answer: Amazon Bedrock\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = claude(\"Which AWS service provides access to foundation models?\")\n",
    "run_standard_and_react(llm, agent, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1c1d0-fa5b-431c-a0ef-7eab9d473b68",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Try solving similar knowledge-based tasks.</b></br> For example, you can ask the model to tell you facts about famous actors, scientists, singers, or historical characters.</code>\n",
    "</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346f232d-c292-44b5-bbb0-36d58b1c78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021807ee-048f-4b3a-960b-9b69fb7b2390",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a name=\"44\">4.4. Decision-making tasks</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will now leverage the planning capabilities of the agent enhanced with custom web search to ask the model to put together complex plans involving a set of decision. The example below is about deciding where to go on an upcoming impromptu vacation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3bb7f99-4bbc-4125-96f2-5b6156ee7eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Human: Help me make a one-week plan for a last-minute vacation for next week.\n",
       "I live in Munich and I'm considering either Tokyo or Las Vegas.\n",
       "Once you've chosen the best destination, produce a plan with activities for each day.\n",
       "\n",
       "Assistant:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Without ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Here is a suggested one-week itinerary for a last-minute vacation to Tokyo:\n",
       "\n",
       "Day 1: \n",
       "- Fly to Tokyo (flight time approx. 11 hours)\n",
       "- Check into hotel, rest and recover from jet lag\n",
       "\n",
       "Day 2: \n",
       "- Visit Meiji Jingu shrine and walk around Harajuku for shopping and street fashion\n",
       "- Wander through Shibuya and see the famous Shibuya crossing\n",
       "\n",
       "Day 3: \n",
       "- Visit Senso-ji temple in Asakusa and explore Nakamise-dori shopping street\n",
       "- Stroll around Ueno Park and visit the Tokyo National Museum\n",
       "\n",
       "Day 4:\n",
       "- Visit the Imperial Palace gardens and Plaza \n",
       "- Walk around Ginza and do some high-end shopping\n",
       "\n",
       "Day 5: \n",
       "- Visit Tokyo Skytree and enjoy views of the city from the observation decks\n",
       "- Explore Akihabara, the electric town, and check out anime and gaming stores \n",
       "\n",
       "Day 6: \n",
       "- Visit Tsukiji Fish Market and have sushi for breakfast \n",
       "- Relax in Hamarikyu Gardens by the water\n",
       "\n",
       "Day 7:\n",
       "- Visit one of the many onsen hot spring resorts outside Tokyo for relaxation (e.g. Hakone)\n",
       "- Fly back from Tokyo in the evening\n",
       "\n",
       "I chose Tokyo over Las Vegas as it offers much more cultural activities and sights to experience in a week. The plan includes a good mix of major attractions, parks, temples, and neighborhoods to explore Tokyo's unique culture, history, and energy. Let me know if you need any other recommendations!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### With ReAct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to consider factors like travel time, cost, and things to do in each location to pick the best destination.\n",
      "Action: DuckDuckGoSearch General\n",
      "Action Input: travel time munich to tokyo vs las vegas\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mMunich is 7 hours behind of Tokyo. If you are in Munich, the most convenient time to accommodate all parties is between 9:00 am and 11:00 am for a conference call or meeting. In Tokyo, this will be a usual working time of between 4:00 pm and 6:00 pm. If you want to reach out to someone in Tokyo and you are available anytime, you can schedule a ... Las Vegas is 16 hours behind of Tokyo. If you are in Las Vegas, the most convenient time to accommodate all parties is between 5:00 pm and 6:00 pm for a conference call or meeting. In Tokyo, this will be a usual working time of between 9:00 am and 10:00 am. If you want to reach out to someone in Tokyo and you are available anytime, you can ... Pack clothing for all kinds of weather. The weather in Munich can change quickly and requires a diverse selection of accessories. Hot summer afternoons can bring sudden downpours or thunderstorms, while crisp winter mornings often call for gloves and sunglasses. Check the forecast before you travel and pack for all eventualities. The best we can do with the meeting planner is to expand the range to cover from 8:00 AM to 6:00 PM your time (Las Vegas, NV). To schedule a conference call or plan a meeting at the best time for both parties, you should try between 4:00 PM and 6:00 PM your time in Las Vegas, NV. That will end up being between 8:00 AM and 10:00 AM in Tokyo, Japan. New York Jets: NFL Week 10 Game Time and Channel. Las Vegas Raiders wide receiver Tre Tucker (11) will be a player to watch in the Raiders' Week 10 matchup with the New York Jets. AP. The vibes in ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Tokyo would have a longer travel time but more cultural activities, while Las Vegas would be shorter but more focused on gambling and shows. Cost is likely similar.\n",
      "Action: wikipedia \n",
      "Action Input: things to do in tokyo for 1 week\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Stranger Things\n",
      "Summary: Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers for Netflix. Produced by Monkey Massacre Productions and 21 Laps Entertainment, the first season was released on Netflix on July 15, 2016. The second and third season followed in October 2017 and July 2019 respectively, and the fourth season aired in two parts in May and July 2022. In February 2022, Stranger Things was renewed for a fifth and final season.\n",
      "Set in the 1980s, the series centers around the residents of the fictional small town of Hawkins, Indiana, as they are plagued by a hostile alternate dimension known as the Upside Down, after a nearby human experimentation facility opens a gateway between it and the normal world. The ensemble cast includes Winona Ryder, David Harbour, Finn Wolfhard, Millie Bobby Brown, Gaten Matarazzo, Caleb McLaughlin, Natalia Dyer, Charlie Heaton, Cara Buono, Matthew Modine, Noah Schnapp, Sadie Sink, Joe Keery, Dacre Montgomery, Sean Astin, Paul Reiser, Maya Hawke, Priah Ferguson and Brett Gelman.\n",
      "The Duffer Brothers developed Stranger Things as a mix of investigative drama and supernatural elements portrayed with horror and childlike sensibilities, while infusing references to the pop culture of the 1980s. Several thematic and directorial elements were inspired by the works of Steven Spielberg, John Carpenter, David Lynch, Stephen King, Wes Craven and H. P. Lovecraft. They also took inspiration from experiments conducted during the Cold War and conspiracy theories involving secret government programs.\n",
      "One of Netflix's flagship series, Stranger Things has attracted record viewership on the streaming platform. It has been critically acclaimed for its characterization, atmosphere, acting, soundtrack, directing, writing, and homages to 1980s films. It has received numerous nominations and awards. An animated spin-off series, developed by Eric Robles and produced by Flying Bark Productions, is in development.\n",
      "\n",
      "Page: Tokyo\n",
      "Summary: Tokyo (; Japanese: 東京, Tōkyō, [toːkʲoː] ), officially the Tokyo Metropolis (東京都, Tōkyō-to), is the capital of Japan and the most populous city in the world with a population of over 14 million residents as of 2023.The Tokyo metropolitan area, which includes Tokyo and three neighboring Japanese prefectures, Tochigi, Gunma, and Ibaraki, is 13,452 square kilometers or 5,194 square miles and the world's most-populous metropolitan area with 40.8 million residents as of 2023.Located at the head of Tokyo Bay, Tokyo is part of the Kantō region on the central coast of Honshu, Japan's largest island. Tokyo serves as Japan's economic center and the seat of both the Japanese government and the Emperor of Japan.\n",
      "Prior to the 17th century, Tokyo was predominantly a fishing village and was named Edo. In 1603, however, the city ascended to political prominence after being named the seat of the Tokugawa shogunate. By the mid-18th century, Edo emerged as one of the world's most-populous cities with a population of over one million people. Following the Meiji Restoration in 1868, the imperial capital in Kyoto was moved to Edo, and the city was renamed Tokyo (lit. 'Eastern Capital'). In 1923, Tokyo was damaged substantially by the Great Kanttō earthquake and was later decimated by allied bombing raids by the during World War II in retailiation for Japan's attack on Pearl Harbor, which then U.S. President Franklin D. Roosevelt famously called a day which will live in infamy. \n",
      "Since 1943, the Tokyo Metropolitan Government has administered the prefecture's special wards, including Tokyo City), various commuter towns and suburbs in its western area, and two outlying island chains known as the Tokyo Islands.\n",
      "Beginning in the mid-20th century, Tokyo underwent rapid reconstruction and expansion that contributed to the era's so-called Japanese economic miracle in which Japan's economy propelled to the second-largest in the world behind the United States.Tok\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m There are so many things to do in Tokyo like shrines, temples, parks, palaces, shopping districts, museums, and more. With a week there I could easily fill each day with activities.\n",
      "Action: wikipedia\n",
      "Action Input: 1 week plan for tokyo\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Tokyo\n",
      "Summary: Tokyo (; Japanese: 東京, Tōkyō, [toːkʲoː] ), officially the Tokyo Metropolis (東京都, Tōkyō-to), is the capital of Japan and the most populous city in the world with a population of over 14 million residents as of 2023.The Tokyo metropolitan area, which includes Tokyo and three neighboring Japanese prefectures, Tochigi, Gunma, and Ibaraki, is 13,452 square kilometers or 5,194 square miles and the world's most-populous metropolitan area with 40.8 million residents as of 2023.Located at the head of Tokyo Bay, Tokyo is part of the Kantō region on the central coast of Honshu, Japan's largest island. Tokyo serves as Japan's economic center and the seat of both the Japanese government and the Emperor of Japan.\n",
      "Prior to the 17th century, Tokyo was predominantly a fishing village and was named Edo. In 1603, however, the city ascended to political prominence after being named the seat of the Tokugawa shogunate. By the mid-18th century, Edo emerged as one of the world's most-populous cities with a population of over one million people. Following the Meiji Restoration in 1868, the imperial capital in Kyoto was moved to Edo, and the city was renamed Tokyo (lit. 'Eastern Capital'). In 1923, Tokyo was damaged substantially by the Great Kanttō earthquake and was later decimated by allied bombing raids by the during World War II in retailiation for Japan's attack on Pearl Harbor, which then U.S. President Franklin D. Roosevelt famously called a day which will live in infamy. \n",
      "Since 1943, the Tokyo Metropolitan Government has administered the prefecture's special wards, including Tokyo City), various commuter towns and suburbs in its western area, and two outlying island chains known as the Tokyo Islands.\n",
      "Beginning in the mid-20th century, Tokyo underwent rapid reconstruction and expansion that contributed to the era's so-called Japanese economic miracle in which Japan's economy propelled to the second-largest in the world behind the United States.Tokyo, which is categorized as an Alpha+ city by the Globalization and World Cities Research Network, is Japan's leading business hub and part of an industrial region that includes Yokohama, Kawasaki, and Chiba. As of 2023, the city is home to 29 of the world's largest 500 companies listed in the annual Fortune Global 500.In 2020, Tokyo ranked fourth on the Global Financial Centres Index behind New York City, London, and Shanghai. The city is home to the world's tallest tower, Tokyo Skytree, and the world's largest underground floodwater diversion facility, the Metropolitan Area Outer Underground Discharge Channel in Kasukabe, Saitama, a Tokyo suburb. The Tokyo Metro Ginza Line, which opened in 1927, is the oldest underground metro line in East Asia. Tokyo is recognized as one of the world's most livable cities; it was ranked fourth in the world in Global Livability Ranking, published in 2021.In the 20th and 21st centuries, Tokyo has hosted several major international events, including the 1964 Summer Olympics and 1964 Summer Paralympics, the 2020 Summer Olympics, and three G7 summits in 1979, 1986, and 1993. Tokyo is an international research and development hub and an academic center with several major universities, including the University of Tokyo. Tokyo Station is the central hub for the Shinkansen, Japan's high-speed railway network, and Shinjuku Station in Tokyo is the world's busiest train station. Notable special wards in Tokyo include Chiyoda, the site of the National Diet Building and the Tokyo Imperial Palace, Shinjuku, the city's administrative center, and Shibuya, a commercial, cultural, and business hub in the city.\n",
      "\n",
      "Page: Haneda Airport\n",
      "Summary: Haneda Airport (羽田空港, Haneda Kūkō), officially Tokyo International Airport (東京国際空港, Tōkyō Kokusai Kūkō), and sometimes referred to as Tokyo Haneda Airport or Haneda International Airport (IATA: HND, ICAO: RJTT), is one of two international airports serving the Greater Tokyo Area, the other one being Narita International Airport\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Here is a suggested 1-week itinerary for Tokyo:\n",
      "\n",
      "Day 1: Arrive in Tokyo, check into hotel. Explore the nearby Asakusa area, visit Senso-ji Temple and walk around Asakusa shopping street in the afternoon. \n",
      "\n",
      "Day 2: Visit Meiji Jingu shrine and walk around Harajuku and Omotesando shopping streets. In the afternoon, visit the Imperial Palace East Garden. \n",
      "\n",
      "Day 3: Visit Tokyo Skytree and surrounding area in the morning. In the afternoon, visit Ueno Park and the Tokyo National Museum. \n",
      "\n",
      "Day 4: Visit the Ginza shopping district and Tsukiji Fish Market in the morning. In the afternoon, visit Roppongi Hills and Tokyo Midtown areas. \n",
      "\n",
      "Day 5: Take a day trip to Kamakura and visit the Great Buddha and Hokoku-ji Temple. Return to Tokyo in the evening. \n",
      "\n",
      "Day 6: Visit teamLab Borderless digital art museum in the morning. In the afternoon, visit Odaiba area and see the Rainbow Bridge and Statue of Liberty replica. \n",
      "\n",
      "Day 7: Morning at your leisure. Transfer to Haneda Airport for evening flight back home.\n",
      "\n",
      "Thought: This itinerary utilizes most days and highlights many top attractions in Tokyo that represent the culture, history, nature, shopping and city life.\n",
      "\n",
      "Final Answer: Based on the information researched, I would recommend Tokyo as the destination for the one-week last-minute vacation next week. The plan outlined above provides a good mix of major sightseeing spots and activities for each day, fully utilizing the time available and showcasing different aspects of Tokyo. While Las Vegas would be shorter travel time, Tokyo offers much more cultural immersion and things to see and do over the course of a week.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the information researched, I would recommend Tokyo as the destination for the one-week last-minute vacation next week. The plan outlined above provides a good mix of major sightseeing spots and activities for each day, fully utilizing the time available and showcasing different aspects of Tokyo. While Las Vegas would be shorter travel time, Tokyo offers much more cultural immersion and things to see and do over the course of a week."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruction = \"\"\"Help me make a one-week plan for a last-minute vacation for next week.\n",
    "I live in Munich and I'm considering either Tokyo or Las Vegas.\n",
    "Once you've chosen the best destination, produce a plan with activities for each day.\"\"\"\n",
    "\n",
    "prompt = claude(instruction)\n",
    "run_standard_and_react(llm, agent, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b78d15-5fd9-436a-9d63-abb980c34e6e",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Try solving similar planning tasks.</b><br/>For example, you can ask the model to plan a party and make a shopping list to be ordered in Amazon.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c97d31d-440c-4c34-8eea-43e599f8fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc138d-23bf-4bfe-9e3e-58d10f85e58e",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have examined a series of tasks that are typically challenging for LLMs to solve, as they involve time awareness, compute capabilities, factual knowledge about current and past persons and events, and the ability to reason and make decisions. The ReAct approach allows the model to generate verbal reasoning traces and actions for a task. This helps make LLM responses more explainable and reduces hallucinations compared to CoT, giving users a better chance of solving the tasks they want.\n",
    "\n",
    "It is important to keep in mind that current agents relies on natural language as an interface between LLMs and additional tools. However, the **reliability of model outputs with ReAct is questionable**, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, a great deal of work needs to be invested in crafting appropriate prompts and in parsing model output correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953e3fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"./images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Well done on completing the lab. Now it's time for you to get creative.</b></p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Try getting the model to solve other tasks for you.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\">You can for instance incorporate finance questions using  <code>YahooFinanceNewsTool</code> or medical informating using the <code>PubmedQueryRun</code> as tools.</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7873a3dd-8d5b-454b-8da8-09e57aabd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee696c1",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
