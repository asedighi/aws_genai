{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6496bcb-59a1-4041-bf86-0a19df0a6765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from datasets) (6.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, multidict, fsspec, frozenlist, async-timeout, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.21.4 multidict-6.0.5 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f1b962-791e-42d3-af83-9187a2a62ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import disable_caching\n",
    "\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "dataset_loaded_sciq = False\n",
    "dataset_loaded_squad = False\n",
    "\n",
    "# creating some dummy value for the dataset global variable\n",
    "dataset = { 'train': [\"a\", \"b\"], 'test':[\"a\", \"b\"]}\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafd2cea-16a0-4114-a849-6d4cf31081ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1080ec2f5d34542aaf83c34e0761f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.99M/3.99M [00:00<00:00, 11.3MB/s]\n",
      "Downloading data: 100%|██████████| 339k/339k [00:00<00:00, 1.65MB/s]\n",
      "Downloading data: 100%|██████████| 343k/343k [00:00<00:00, 3.25MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c61970458f4fefa307045e0d328750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e70c5d8705409aaf6003b005bc8c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90cac1e4e1a4d56b39c63d8ef37e61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Load data\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Train dataset size 4998 with columns['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support']\n",
      "Validation dataset size 6000 with columns['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support']\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create DAFT dataset\n",
      "---------------------------------------------------------------------------------------------------\n",
      "DAFT dataset example:  \n",
      "The skeletal system is made up of bones, cartilage, and ligaments. The skeletal system has many important functions in your body. What bones protect the heart and lungs? What protects the brain?. \n",
      "The force exerted by circulating blood on the walls of blood vessels is called blood pressure . Blood pressure is highest in arteries and lowest in veins. When you have your blood pressure checked, it is the blood pressure in arteries that is measured. High blood pressure, or hypertension , is a serious health risk but can often be controlled with lifestyle changes or medication. You can learn more about hypertension by watching the animation at this link: http://www. healthcentral. com/high-blood-pressure/introduction-47-115. html . \n",
      "Bacteria are the most diverse and abundant group of organisms on Earth. They live in almost all environments. They are found in the ocean, the soil, and the intestines of animals. They are even found in rocks deep below Earth’s surface. Any surface that has no\n",
      "Export DAFT dataset: dataset_finetune_daft.txt\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create IST dataset\n",
      "---------------------------------------------------------------------------------------------------\n",
      "question          IST dataframe example: What part of a plant pr...\n",
      "correct_answer                          IST dataframe example: wall\n",
      "Name: 0, dtype: object\n",
      "IST dataset example:  {'question': 'What part of a plant protects the plant cell, maintains its shape, and prevents excessive uptake of water?', 'correct_answer': 'wall'}\n",
      "Exoprt IST dataset: dataset_finetune_ist.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2967022e76964309a625ba86299cdaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create prompt template and store to template.json\n",
      "---------------------------------------------------------------------------------------------------\n",
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: Answer this question:\\n{question}\\n', 'completion': '{correct_answer}'}\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create evaluation dataset\n",
      "---------------------------------------------------------------------------------------------------\n",
      "model_input      Evaluation dataframe example: Below is an inst...\n",
      "target_output               Evaluation dataframe example: oxidants\n",
      "Name: 0, dtype: object\n",
      "Evaluation dataset example:  {'model_input': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: Answer this question:\\nCompounds that are capable of accepting electrons, such as o 2 or f2, are called what?', 'target_output': 'oxidants'}\n",
      "Export Evaluation dataset: dataset_evaluation.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfe529550b6475a8a23ed959a611f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create evaluation small dataset\n",
      "model_input      Evaluation dataframe small example: Below is a...\n",
      "target_output         Evaluation dataframe small example: oxidants\n",
      "Name: 0, dtype: object\n",
      "Evaluation dataset small example:  {'model_input': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCompounds that are capable of accepting electrons, such as o 2 or f2, are called what?\\n\\n### Input:\\nOxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F2, are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.', 'target_output': 'oxidants'}\n",
      "Export Evaluation dataset: dataset_evaluation_small.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1ffceab7404729b5ed15fe7235222f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "Create evaluation HIL dataset\n",
      "prompt               Evaluation dataframe hil example: Below is an ...\n",
      "referenceResponse           Evaluation dataframe hil example: oxidants\n",
      "Name: 0, dtype: object\n",
      "Evaluation dataset hil example:  {'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCompounds that are capable of accepting electrons, such as o 2 or f2, are called what?\\n\\n### Input:\\nOxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F2, are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.', 'referenceResponse': 'oxidants'}\n",
      "Export Evaluation dataset: dataset_evaluation_hil.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8883f03dc94649b5223a10b39fc238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Uploading custom template...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Done\n",
      "Uploading instruction tuning dataset...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Fine-tuning ist data: s3://sagemaker-us-east-1-388338269460/datasets/sciq/fine_tuning/instruction_fine_tuning\n",
      "Uploading domain adaptation tuning dataset...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Fine-tuning daft data: s3://sagemaker-us-east-1-388338269460/datasets/sciq/fine_tuning/domain_adaptation_fine_tuning\n",
      "Uploading evaluation dataset...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Evaluation data: s3://sagemaker-us-east-1-388338269460/datasets/sciq/evaluation/automatic\n",
      "Uploading small evaluation dataset...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Evaluation data: s3://sagemaker-us-east-1-388338269460/datasets/sciq/evaluation/automatic_small\n",
      "Uploading HIL evaluation dataset...\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Evaluation data: s3://sagemaker-us-east-1-388338269460/datasets/sciq/evaluation/hil\n"
     ]
    }
   ],
   "source": [
    "def print_dashes():\n",
    "    print('-'.join('' for x in range(100)))\n",
    "\n",
    "def translate_to_text(data, column_name=\"context\"):\n",
    "    data_pd = pd.DataFrame(data)\n",
    "    return \" \\n\".join(((data_pd.drop_duplicates(subset=[column_name]))[column_name]))\n",
    "\n",
    "def safe_open_w(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return open(path, 'w')\n",
    "\n",
    "def write_to_file(input_str, file_path):\n",
    "    with safe_open_w(file_path) as text_file:\n",
    "        text_file.write(input_str)\n",
    "\n",
    "def create_custom_template(template, file_path):\n",
    "    with safe_open_w(file_path) as text_file:\n",
    "        json.dump(template, text_file)\n",
    "    return template\n",
    "\n",
    "def attach_cors_to_bucket(bucket_name):    \n",
    "    s3 = boto3.client('s3')\n",
    "  \n",
    "    try:\n",
    "        response = s3.put_bucket_cors(Bucket = bucket_name, \n",
    "                                      CORSConfiguration = {\n",
    "                                            'CORSRules' : [\n",
    "                                                {\n",
    "                                                    'ID' : bucket_name + 'cors',\n",
    "                                                    'AllowedHeaders' : [ '*' ],\n",
    "                                                    'AllowedMethods' : [ 'PUT', 'GET', 'POST', 'DELETE', 'HEAD' ],\n",
    "                                                    'AllowedOrigins' : [ '*' ],\n",
    "                                                    'ExposeHeaders' :  [ 'ETag', 'x-amz-delete-marker', 'x-amz-server-side-encryption',\n",
    "                                                                         'x-amz-request-id','x-amz-version-id','x-amz-id-2']\n",
    "                                                }\n",
    "                                            ]\n",
    "                                        })\n",
    "    except ClientError as e:\n",
    "            return None\n",
    "    return response\n",
    "\n",
    "def upload_workshop_dataset(dataset_name,\n",
    "                            output_bucket = sagemaker.Session().default_bucket(),\n",
    "                            local_path = \".\"):\n",
    "    \n",
    "    attach_cors_to_bucket(output_bucket) # requirment by Studio UI\n",
    "    \n",
    "    output_s3_path =  output_bucket + \"/datasets\" \n",
    "    \n",
    "    data_location = f\"s3://{output_s3_path}/\" + dataset_name\n",
    "    \n",
    "    fine_tune_data_ist_location = f\"{data_location}/fine_tuning/instruction_fine_tuning\"\n",
    "    fine_tune_data_daft_location = f\"{data_location}/fine_tuning/domain_adaptation_fine_tuning\"\n",
    "    evaluation_data_location = f\"{data_location}/evaluation/automatic\"\n",
    "    evaluation_data_small_location = f\"{data_location}/evaluation/automatic_small\"\n",
    "    evaluation_data_hil_location = f\"{data_location}/evaluation/hil\"\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/template.json\")):\n",
    "        print(\"Uploading custom template...\")\n",
    "        S3Uploader.upload(f\"{local_path}/template.json\", fine_tune_data_ist_location)\n",
    "        print(\"Done\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_finetune_ist.jsonl\")):\n",
    "        print(\"Uploading instruction tuning dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_finetune_ist.jsonl\", fine_tune_data_ist_location)\n",
    "        print(f\"Fine-tuning ist data: {fine_tune_data_ist_location}\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_finetune_daft.txt\")):\n",
    "        print(\"Uploading domain adaptation tuning dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_finetune_daft.txt\", fine_tune_data_daft_location)\n",
    "        print(f\"Fine-tuning daft data: {fine_tune_data_daft_location}\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_evaluation.jsonl\")):\n",
    "        print(\"Uploading evaluation dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_evaluation.jsonl\", evaluation_data_location)\n",
    "        print(f\"Evaluation data: {evaluation_data_location}\")\n",
    "    \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_evaluation_small.jsonl\")):\n",
    "        print(\"Uploading small evaluation dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_evaluation_small.jsonl\", evaluation_data_small_location)\n",
    "        print(f\"Evaluation data: {evaluation_data_small_location}\")\n",
    "        \n",
    "    if(os.path.isfile(f\"{local_path}/dataset_evaluation_hil.jsonl\")):\n",
    "        print(\"Uploading HIL evaluation dataset...\")\n",
    "        S3Uploader.upload(f\"{local_path}/dataset_evaluation_hil.jsonl\", evaluation_data_hil_location)\n",
    "        print(f\"Evaluation data: {evaluation_data_hil_location}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_sciq():\n",
    "    dataset_name = 'sciq'\n",
    "\n",
    "    # required for ipython shell\n",
    "    global dataset_loaded_sciq \n",
    "    global dataset_loaded_squad \n",
    "    global dataset\n",
    "    \n",
    "    print (dataset_loaded_sciq)\n",
    "    \n",
    "    if(dataset_loaded_sciq == False):\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        dataset_loaded_sciq = True\n",
    "   \n",
    "    dataset_training_df = pd.DataFrame(dataset['train'])\n",
    "    dataset_validation_df = pd.DataFrame(dataset['test'])\n",
    "    \n",
    "    number_of_raws_training = 5000 # dataset_training_df.size\n",
    "    dataset_training_df = dataset_training_df.sample(n=int(number_of_raws_training/len(dataset_training_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    #number_of_raws_validation = 2000 # dataset_training_df.size\n",
    "    #dataset_validation_df = dataset_validation_df.sample(n=int(number_of_raws_validation/len(dataset_validation_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    print_dashes()\n",
    "    print(\"Load data\")\n",
    "    print_dashes()\n",
    "    print(\"Train dataset size \" + str(dataset_training_df.size) + \" with columns\" + str(dataset_training_df.columns.to_list()) )\n",
    "    print(\"Validation dataset size \" + str(dataset_validation_df.size) + \" with columns\" + str(dataset_validation_df.columns.to_list()) )\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate DAFT dataset\")\n",
    "    print_dashes()\n",
    "    data_train_daft = translate_to_text(dataset_training_df, column_name='support')\n",
    "    print(\"DAFT dataset example: \" + data_train_daft[0:1000])\n",
    "    print(\"Export DAFT dataset: dataset_finetune_daft.txt\")\n",
    "    write_to_file(data_train_daft, f\"./{dataset_name}/dataset_finetune_daft.txt\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate IST dataset\")\n",
    "    print_dashes()\n",
    "    \n",
    "    include_context = False\n",
    "    \n",
    "    if include_context:\n",
    "        fields = ['support', 'question', 'correct_answer']\n",
    "        template = {\n",
    "            \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{support}\",\n",
    "            \"completion\": \"{correct_answer}\"\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        fields = ['question', 'correct_answer']\n",
    "        template = {\n",
    "            \"prompt\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: Answer this question:\\n{question}\\n\",\n",
    "            \"completion\": \"{correct_answer}\"\n",
    "        }\n",
    "        \n",
    "    dataset_train_ist_df = dataset_training_df[fields].copy()\n",
    "    print(\"IST dataframe example: \" + dataset_train_ist_df.iloc[0])\n",
    "    dataset_fine_tune_ist = Dataset.from_pandas(dataset_train_ist_df)\n",
    "    print(\"IST dataset example: \",dataset_fine_tune_ist[0])\n",
    "    print(\"Exoprt IST dataset: dataset_finetune_ist.jsonl\")\n",
    "    dataset_fine_tune_ist.to_json(f\"./{dataset_name}/dataset_finetune_ist.jsonl\",orient='records', lines=True)\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate prompt template and store to template.json\")\n",
    "    print_dashes()\n",
    "    print(create_custom_template(template, f\"./{dataset_name}/template.json\"))\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate evaluation dataset\")\n",
    "    print_dashes()\n",
    "    \n",
    "    dataset_validation_with_context_df = dataset_validation_df.copy()\n",
    "    dataset_validation_with_context_df[\"model_input\"] = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\" + dataset_validation_with_context_df[\"question\"] + \"\\n\\n### Input:\\n\" + dataset_validation_with_context_df[\"support\"]\n",
    "    dataset_validation_with_context_df = dataset_validation_with_context_df[['model_input','correct_answer']].copy()\n",
    "    dataset_validation_with_context_df = dataset_validation_with_context_df.rename(columns={\"correct_answer\": \"target_output\"})\n",
    "    \n",
    "    dataset_validation_no_context_df = dataset_validation_df.copy()\n",
    "    dataset_validation_no_context_df[\"model_input\"] = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: Answer this question:\\n\"+ dataset_validation_no_context_df[\"question\"]\n",
    "    dataset_validation_no_context_df = dataset_validation_no_context_df[['model_input','correct_answer']].copy()\n",
    "    dataset_validation_no_context_df = dataset_validation_no_context_df.rename(columns={\"correct_answer\": \"target_output\"})\n",
    "    \n",
    "    if include_context:\n",
    "        print(\"Evaluation dataframe example: \" + dataset_validation_with_context_df.iloc[0])\n",
    "        dataset_evaluation = Dataset.from_pandas(dataset_validation_with_context_df)\n",
    "        print(\"Evaluation dataset example: \", dataset_evaluation[0])\n",
    "    else:\n",
    "        print(\"Evaluation dataframe example: \" + dataset_validation_no_context_df.iloc[0])\n",
    "        dataset_evaluation = Dataset.from_pandas(dataset_validation_no_context_df)\n",
    "        print(\"Evaluation dataset example: \", dataset_evaluation[0])\n",
    "    print(\"Export Evaluation dataset: dataset_evaluation.jsonl\")\n",
    "    dataset_evaluation.to_json(f\"./{dataset_name}/dataset_evaluation.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    # making the evaluation_small dataset \n",
    "    print(\"\\nCreate evaluation small dataset\")\n",
    "    \n",
    "    dataset_evaluation_small_df = dataset_validation_with_context_df.head(10)\n",
    "    print(\"Evaluation dataframe small example: \" + dataset_evaluation_small_df.iloc[0])\n",
    "    dataset_evaluation_small = Dataset.from_pandas(dataset_evaluation_small_df)\n",
    "    print(\"Evaluation dataset small example: \", dataset_evaluation_small[0])\n",
    "    print(\"Export Evaluation dataset: dataset_evaluation_small.jsonl\")\n",
    "    dataset_evaluation_small.to_json(f\"./{dataset_name}/dataset_evaluation_small.jsonl\")\n",
    "    print_dashes()\n",
    "\n",
    "    # making the evaluation_hil dataset \n",
    "    print(\"\\nCreate evaluation HIL dataset\")\n",
    "    \n",
    "    dataset_evaluation_hil_df = dataset_validation_with_context_df.head(10)\n",
    "    dataset_evaluation_hil_df = dataset_evaluation_hil_df.rename(columns={\"model_input\": \"prompt\", \"target_output\": \"referenceResponse\"})\n",
    "    print(\"Evaluation dataframe hil example: \" + dataset_evaluation_hil_df.iloc[0])\n",
    "    dataset_evaluation_hil = Dataset.from_pandas(dataset_evaluation_hil_df)\n",
    "    print(\"Evaluation dataset hil example: \", dataset_evaluation_hil[0])\n",
    "    print(\"Export Evaluation dataset: dataset_evaluation_hil.jsonl\")\n",
    "    dataset_evaluation_hil.to_json(f\"./{dataset_name}/dataset_evaluation_hil.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    upload_workshop_dataset(dataset_name = dataset_name, output_bucket = sagemaker.Session().default_bucket(), local_path = f\"./{dataset_name}\")\n",
    "\n",
    "def prepare_dataset_squad():\n",
    "    \n",
    "    dataset_name = 'squad'\n",
    "\n",
    "    # required for ipython shell\n",
    "    global dataset_loaded_sciq \n",
    "    global dataset_loaded_squad \n",
    "    global dataset\n",
    "    \n",
    "    if(dataset_loaded_squad == False):\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        dataset_loaded_squad = True\n",
    "        \n",
    "    dataset_training_df = pd.DataFrame(dataset['train'])\n",
    "    dataset_validation_df = pd.DataFrame(dataset['validation'])\n",
    "    \n",
    "    #number_of_raws_training = 5000 # dataset_training_df.size\n",
    "    #dataset_training_df = dataset_training_df.sample(n=int(number_of_raws_training/len(dataset_training_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    #number_of_raws_validation = 2000 # dataset_training_df.size\n",
    "    #dataset_validation_df = dataset_validation_df.sample(n=int(number_of_raws_validation/len(dataset_validation_df.columns)), random_state=42, ignore_index=True)\n",
    "    \n",
    "    print_dashes()\n",
    "    print(\"Load data\")\n",
    "    print_dashes()\n",
    "    print(\"Train dataset size \" + str(dataset_training_df.size) + \" with columns\" + str(dataset_training_df.columns.to_list()) )\n",
    "    print(\"Validation dataset size \" + str(dataset_validation_df.size) + \" with columns\" + str(dataset_validation_df.columns.to_list()) )\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate DAFT dataset\")\n",
    "    print_dashes()\n",
    "    data_train_daft = translate_to_text(dataset_training_df)\n",
    "    print(\"DAFT dataset example: \" + data_train_daft[0:1000])\n",
    "    print(\"Exoprt DAFT dataset: dataset_finetune_daft.txt\")\n",
    "    write_to_file(data_train_daft, f\"./{dataset_name}/dataset_finetune_daft.txt\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate IST dataset\")\n",
    "    print_dashes()\n",
    "    dataset_train_ist_df = dataset_training_df[['context', 'question', 'answers']].copy()\n",
    "    dataset_train_ist_df['answers'] = dataset_train_ist_df['answers'].apply(lambda x: str(x[\"text\"][0]))\n",
    "    print(\"IST dataframe example: \" + dataset_train_ist_df.iloc[0])\n",
    "    dataset_fine_tune_ist = Dataset.from_pandas(dataset_train_ist_df)\n",
    "    print(\"IST dataset example: \",dataset_fine_tune_ist[0])\n",
    "    print(\"Exoprt IST dataset: dataset_finetune_ist.jsonl\")\n",
    "    dataset_fine_tune_ist.to_json(f\"./{dataset_name}/dataset_finetune_ist.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate prompt template and store to template.json\")\n",
    "    print_dashes()\n",
    "    template = {\n",
    "          \"prompt\": \"Given the following context: {context}\\n\\nCould you answer this question: {question} \",\n",
    "          \"completion\": \"{answers}\"\n",
    "        }\n",
    "    print(create_custom_template(template, f\"./{dataset_name}/template.json\"))\n",
    "    print_dashes()\n",
    "    \n",
    "    print(\"\\nCreate evaluation dataset\")\n",
    "    print_dashes()\n",
    "    dataset_validation_df[\"model_input\"] = \"Given the following context:\" + dataset_validation_df[\"context\"] + \"\\n\\nCould you answer this question: \" + dataset_validation_df[\"question\"]\n",
    "    dataset_evaluation_df = dataset_validation_df[['model_input','answers']].copy()\n",
    "    dataset_evaluation_df['answers'] = dataset_evaluation_df['answers'].apply(lambda x: str(' <OR> '.join(data for data in x[\"text\"])))\n",
    "    dataset_evaluation_df = dataset_evaluation_df.rename(columns={\"answers\": \"target_output\"})\n",
    "    print(\"Evaluation dataframe example: \" + dataset_evaluation_df.iloc[0])\n",
    "    dataset_evaluation = Dataset.from_pandas(dataset_evaluation_df)\n",
    "    print(\"Evaluation dataset example: \", dataset_evaluation[0])\n",
    "    print(\"Exoprt Evaluation dataset: dataset_evaluation.jsonl\")\n",
    "    dataset_evaluation.to_json(f\"./{dataset_name}/dataset_evaluation.jsonl\")\n",
    "    print_dashes()\n",
    "\n",
    "    print(\"\\nCreate evaluation small dataset\")\n",
    "    dataset_evaluation_small_df = dataset_evaluation_df.head(10)\n",
    "    print(\"Evaluation dataframe small example: \" + dataset_evaluation_small_df.iloc[0])\n",
    "    dataset_evaluation_small = Dataset.from_pandas(dataset_evaluation_small_df)\n",
    "    print(\"Evaluation dataset small example: \", dataset_evaluation_small[0])\n",
    "    print(\"Export Evaluation dataset: dataset_evaluation_small.jsonl\")\n",
    "    dataset_evaluation_small.to_json(f\"./{dataset_name}/dataset_evaluation_small.jsonl\")\n",
    "    print_dashes()\n",
    "\n",
    "    # making the evaluation_hil dataset \n",
    "    print(\"\\nCreate evaluation HIL dataset\")\n",
    "    \n",
    "    dataset_evaluation_hil_df = dataset_evaluation_df.head(10)\n",
    "    dataset_evaluation_hil_df = dataset_evaluation_hil_df.rename(columns={\"model_input\": \"prompt\", \"target_output\": \"referenceResponse\"})\n",
    "    print(\"Evaluation dataframe hil example: \" + dataset_evaluation_hil_df.iloc[0])\n",
    "    dataset_evaluation_hil = Dataset.from_pandas(dataset_evaluation_hil_df)\n",
    "    print(\"Evaluation dataset hil example: \", dataset_evaluation_hil[0])\n",
    "    print(\"Export Evaluation dataset: dataset_evaluation_hil.jsonl\")\n",
    "    dataset_evaluation_hil.to_json(f\"./{dataset_name}/dataset_evaluation_hil.jsonl\")\n",
    "    print_dashes()\n",
    "    \n",
    "    upload_workshop_dataset(dataset_name = dataset_name, output_bucket = sagemaker.Session().default_bucket(), local_path = f\"./{dataset_name}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_dataset_sciq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8cf27b-a3a1-4612-8e0f-31ab98970295",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13a92d-89ed-4624-8196-5e98aeebe1e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
